import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import TypedDict, List, Optional, Union, Dict, Any, Literal
import json
import re
import asyncio
import logging
import time
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import deque, defaultdict
import threading

# æ·»åŠ æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)

@dataclass
class MemoryItem:
    """è®°å¿†é¡¹ç›®æ•°æ®ç»“æ„"""
    content: str
    timestamp: datetime
    importance: float  # é‡è¦æ€§æƒé‡ 0-1
    context_type: str  # "conversation", "document", "fact", "preference"
    user_id: Optional[str] = None
    document_id: Optional[str] = None
    session_id: Optional[str] = None
    access_count: int = 0
    last_accessed: Optional[datetime] = None

@dataclass 
class ConversationMemory:
    """å¯¹è¯è®°å¿†ç®¡ç†"""
    short_term: deque = field(default_factory=lambda: deque(maxlen=50))  # çŸ­æœŸè®°å¿†
    long_term: Dict[str, MemoryItem] = field(default_factory=dict)  # é•¿æœŸè®°å¿†
    session_memory: Dict[str, List[MemoryItem]] = field(default_factory=dict)  # ä¼šè¯è®°å¿†
    user_preferences: Dict[str, Any] = field(default_factory=dict)  # ç”¨æˆ·åå¥½
    document_insights: Dict[str, List[MemoryItem]] = field(default_factory=dict)  # æ–‡æ¡£æ´å¯Ÿ

@dataclass
class IntentAnalysisConfig:
    """æ„å›¾åˆ†æé…ç½®"""
    use_advanced_analysis: bool = True
    confidence_threshold: float = 0.7
    enable_context_awareness: bool = True
    enable_document_awareness: bool = True
    enable_multi_intent: bool = False
    cache_intent_results: bool = True
    intent_cache_ttl: int = 300  # 5åˆ†é’Ÿ

class SourceInfo(TypedDict):
    line_number: int
    content: str
    page: int
    start_pos: int
    end_pos: int
    is_scanned: bool
    similarity: float
    document_id: Optional[str]
    document_name: Optional[str]

class MessageContent(TypedDict):
    type: Literal["text", "markdown", "code", "table", "image"]
    content: str
    language: Optional[str]  # For code blocks
    alt: Optional[str]  # For images
    columns: Optional[List[str]]  # For tables
    rows: Optional[List[List[Any]]]  # For tables
    metadata: Optional[Dict[str, Any]]  # Additional metadata

class AIResponse(TypedDict):
    answer: str  # Backward compatibility
    sources: List[SourceInfo]
    confidence: float
    reply: List[MessageContent]  # New rich content format

class MessageInfo(TypedDict):
    role: str
    content: str

class AdvancedMemoryManager:
    """é«˜çº§è®°å¿†ç®¡ç†å™¨ - å‚è€ƒé•¿æœŸè®°å¿†é¡¹ç›®çš„æ¦‚å¿µ"""
    
    def __init__(self, max_short_term: int = 50, max_long_term: int = 1000):
        self.memory = ConversationMemory()
        self.max_short_term = max_short_term
        self.max_long_term = max_long_term
        self.lock = threading.RLock()
        
        # è®°å¿†é‡è¦æ€§è®¡ç®—æƒé‡
        self.importance_weights = {
            "recency": 0.3,      # æ—¶é—´æ–°è¿‘æ€§
            "frequency": 0.2,    # è®¿é—®é¢‘ç‡
            "relevance": 0.3,    # å†…å®¹ç›¸å…³æ€§
            "explicit": 0.2      # æ˜¾å¼é‡è¦æ€§æ ‡è®°
        }
        
        logger.info(f"[MEMORY_INIT] è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ– - çŸ­æœŸå®¹é‡: {max_short_term}, é•¿æœŸå®¹é‡: {max_long_term}")

    def add_memory(self, content: str, context_type: str = "conversation", 
                   importance: float = 0.5, user_id: str = None, 
                   document_id: str = None, session_id: str = None) -> str:
        """æ·»åŠ è®°å¿†é¡¹ç›®"""
        with self.lock:
            memory_id = f"mem_{int(time.time() * 1000)}"
            memory_item = MemoryItem(
                content=content,
                timestamp=datetime.utcnow(),
                importance=importance,
                context_type=context_type,
                user_id=user_id,
                document_id=document_id,
                session_id=session_id
            )
            
            # æ·»åŠ åˆ°çŸ­æœŸè®°å¿†
            self.memory.short_term.append(memory_item)
            
            # æ ¹æ®é‡è¦æ€§å†³å®šæ˜¯å¦è¿›å…¥é•¿æœŸè®°å¿†
            if importance > 0.7 or context_type in ["fact", "preference"]:
                self.memory.long_term[memory_id] = memory_item
                self._manage_long_term_capacity()
            
            # æ·»åŠ åˆ°ä¼šè¯è®°å¿†
            if session_id:
                if session_id not in self.memory.session_memory:
                    self.memory.session_memory[session_id] = []
                self.memory.session_memory[session_id].append(memory_item)
            
            # æ·»åŠ åˆ°æ–‡æ¡£æ´å¯Ÿ
            if document_id:
                if document_id not in self.memory.document_insights:
                    self.memory.document_insights[document_id] = []
                self.memory.document_insights[document_id].append(memory_item)
            
            logger.debug(f"[MEMORY_ADD] æ·»åŠ è®°å¿† - ç±»å‹: {context_type}, é‡è¦æ€§: {importance:.2f}")
            return memory_id

    def get_relevant_memories(self, query: str, user_id: str = None, 
                            session_id: str = None, limit: int = 5) -> List[MemoryItem]:
        """è·å–ç›¸å…³è®°å¿†"""
        with self.lock:
            relevant_memories = []
            
            # ä»çŸ­æœŸè®°å¿†ä¸­æŸ¥æ‰¾
            for memory in list(self.memory.short_term):
                if self._is_memory_relevant(memory, query, user_id, session_id):
                    memory.access_count += 1
                    memory.last_accessed = datetime.utcnow()
                    relevant_memories.append(memory)
            
            # ä»é•¿æœŸè®°å¿†ä¸­æŸ¥æ‰¾
            for memory in self.memory.long_term.values():
                if self._is_memory_relevant(memory, query, user_id, session_id):
                    memory.access_count += 1
                    memory.last_accessed = datetime.utcnow()
                    relevant_memories.append(memory)
            
            # æŒ‰é‡è¦æ€§å’Œç›¸å…³æ€§æ’åº
            relevant_memories.sort(key=lambda m: self._calculate_memory_score(m, query), reverse=True)
            
            logger.debug(f"[MEMORY_RETRIEVE] æ£€ç´¢åˆ° {len(relevant_memories)} æ¡ç›¸å…³è®°å¿†")
            return relevant_memories[:limit]

    def _is_memory_relevant(self, memory: MemoryItem, query: str, user_id: str = None, session_id: str = None) -> bool:
        """åˆ¤æ–­è®°å¿†æ˜¯å¦ç›¸å…³"""
        # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æŸ¥
        query_words = set(query.lower().split())
        memory_words = set(memory.content.lower().split())
        
        if len(query_words & memory_words) > 0:
            return True
        
        # ç”¨æˆ·å’Œä¼šè¯åŒ¹é…
        if user_id and memory.user_id == user_id:
            return True
        
        if session_id and memory.session_id == session_id:
            return True
        
        return False

    def _calculate_memory_score(self, memory: MemoryItem, query: str) -> float:
        """è®¡ç®—è®°å¿†å¾—åˆ†"""
        score = 0.0
        
        # æ—¶é—´æ–°è¿‘æ€§
        age_hours = (datetime.utcnow() - memory.timestamp).total_seconds() / 3600
        recency_score = max(0, 1 - age_hours / (24 * 7))  # ä¸€å‘¨å†…çš„è®°å¿†å¾—åˆ†è¾ƒé«˜
        score += recency_score * self.importance_weights["recency"]
        
        # è®¿é—®é¢‘ç‡
        frequency_score = min(1.0, memory.access_count / 10)
        score += frequency_score * self.importance_weights["frequency"]
        
        # æ˜¾å¼é‡è¦æ€§
        score += memory.importance * self.importance_weights["explicit"]
        
        # å†…å®¹ç›¸å…³æ€§ï¼ˆç®€å•å®ç°ï¼‰
        query_words = set(query.lower().split())
        memory_words = set(memory.content.lower().split())
        if len(query_words) > 0:
            relevance_score = len(query_words & memory_words) / len(query_words)
            score += relevance_score * self.importance_weights["relevance"]
        
        return score

    def _manage_long_term_capacity(self):
        """ç®¡ç†é•¿æœŸè®°å¿†å®¹é‡"""
        if len(self.memory.long_term) > self.max_long_term:
            # ç§»é™¤æœ€ä¸é‡è¦çš„è®°å¿†
            memories_by_score = sorted(
                self.memory.long_term.items(),
                key=lambda x: self._calculate_memory_score(x[1], ""),
                reverse=False
            )
            
            to_remove = len(self.memory.long_term) - self.max_long_term
            for i in range(to_remove):
                memory_id, _ = memories_by_score[i]
                del self.memory.long_term[memory_id]
            
            logger.debug(f"[MEMORY_CLEANUP] æ¸…ç†äº† {to_remove} æ¡é•¿æœŸè®°å¿†")

class EnhancedIntentAnalyzer:
    """å¢å¼ºçš„æ„å›¾åˆ†æå™¨ - ç»“åˆæ–‡æ¡£æ„ŸçŸ¥å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥"""
    
    def __init__(self, config: IntentAnalysisConfig = None):
        self.config = config or IntentAnalysisConfig()
        self.intent_cache = {}
        self.cache_timestamps = {}
        
        # é¢„å®šä¹‰æ„å›¾æ¨¡å¼
        self.intent_patterns = {
            "DOCUMENT_QUERY": [
                r"æ–‡æ¡£.*è¯´.*ä»€ä¹ˆ", r"æ ¹æ®.*æ–‡æ¡£", r"æ–‡æ¡£.*å†…å®¹", r"åœ¨.*æ–‡æ¡£.*ä¸­",
                r"è¿™.*æ–‡æ¡£.*æè¿°", r"æ–‡æ¡£.*æåˆ°", r"æŸ¥æ‰¾.*æ–‡æ¡£", r"æœç´¢.*æ–‡æ¡£"
            ],
            "GENERAL_QUERY": [
                r"ä»€ä¹ˆæ˜¯", r"å¦‚ä½•.*", r"ä¸ºä»€ä¹ˆ", r"å‘Šè¯‰æˆ‘", r"è§£é‡Š.*",
                r"ä»‹ç».*", r"è¯´æ˜.*", r"æè¿°.*"  
            ],
            "COMPARISON_QUERY": [
                r"æ¯”è¾ƒ.*", r"å¯¹æ¯”.*", r".*åŒºåˆ«.*", r".*å·®å¼‚.*", r".*ä¼˜ç¼ºç‚¹.*"
            ],
            "SUMMARY_QUERY": [
                r"æ€»ç»“.*", r"æ¦‚æ‹¬.*", r"å½’çº³.*", r"æ•´ç†.*", r"æ¢³ç†.*"
            ],
            "ANALYSIS_QUERY": [
                r"åˆ†æ.*", r"è¯„ä¼°.*", r"åˆ¤æ–­.*", r"è¯„ä»·.*", r"æ·±å…¥.*ç ”ç©¶.*"
            ]
        }
        
        logger.info(f"[INTENT_INIT] æ„å›¾åˆ†æå™¨åˆå§‹åŒ– - é«˜çº§åˆ†æ: {self.config.use_advanced_analysis}")

    async def analyze_intent(self, question: str, has_documents: bool = False, 
                           history: str = None, document_info: dict = None,
                           user_context: dict = None) -> str:
        """åˆ†æç”¨æˆ·æ„å›¾ - å¢å¼ºç‰ˆæœ¬"""
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{question}_{has_documents}_{hash(history or '')}"
        if self.config.cache_intent_results and cache_key in self.intent_cache:
            cached_time = self.cache_timestamps.get(cache_key, datetime.min)
            if (datetime.utcnow() - cached_time).seconds < self.config.intent_cache_ttl:
                logger.debug(f"[INTENT_CACHE] ä½¿ç”¨ç¼“å­˜ç»“æœ")
                return self.intent_cache[cache_key]
        
        logger.info(f"[INTENT_ANALYZE] å¼€å§‹æ„å›¾åˆ†æ - é—®é¢˜: {question[:50]}...")
        
        # åŸºç¡€æ¨¡å¼åŒ¹é…
        base_intent = self._pattern_based_analysis(question)
        logger.debug(f"[INTENT_PATTERN] æ¨¡å¼åŒ¹é…ç»“æœ: {base_intent}")
        
        # ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æ
        if self.config.enable_context_awareness and history:
            context_intent = self._context_aware_analysis(question, history)
            logger.debug(f"[INTENT_CONTEXT] ä¸Šä¸‹æ–‡åˆ†æç»“æœ: {context_intent}")
        else:
            context_intent = base_intent
        
        # æ–‡æ¡£æ„ŸçŸ¥åˆ†æ
        if self.config.enable_document_awareness and has_documents:
            doc_intent = self._document_aware_analysis(question, document_info)
            logger.debug(f"[INTENT_DOCUMENT] æ–‡æ¡£æ„ŸçŸ¥åˆ†æç»“æœ: {doc_intent}")
        else:
            doc_intent = context_intent
        
        # æœ€ç»ˆæ„å›¾å†³ç­–
        final_intent = self._make_final_decision(base_intent, context_intent, doc_intent, has_documents)
        
        # ç¼“å­˜ç»“æœ
        if self.config.cache_intent_results:
            self.intent_cache[cache_key] = final_intent
            self.cache_timestamps[cache_key] = datetime.utcnow()
        
        logger.info(f"[INTENT_FINAL] æœ€ç»ˆæ„å›¾: {final_intent}")
        return final_intent

    def _pattern_based_analysis(self, question: str) -> str:
        """åŸºäºæ¨¡å¼çš„æ„å›¾åˆ†æ"""
        question_lower = question.lower()
        
        # é€ä¸ªæ£€æŸ¥æ„å›¾æ¨¡å¼
        for intent, patterns in self.intent_patterns.items():
            for pattern in patterns:
                if re.search(pattern, question_lower):
                    return intent
        
        # é»˜è®¤æ„å›¾
        return "GENERAL_QUERY"

    def _context_aware_analysis(self, question: str, history: str) -> str:
        """ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æ"""
        if not history:
            return self._pattern_based_analysis(question)
        
        # åˆ†æå†å²å¯¹è¯ä¸­çš„æ–‡æ¡£å¼•ç”¨
        if any(keyword in history.lower() for keyword in ["æ–‡æ¡£", "æ ¹æ®", "å†…å®¹", "ææ–™"]):
            return "DOCUMENT_QUERY"
        
        # åˆ†æå¯¹è¯è¿ç»­æ€§
        if any(keyword in question.lower() for keyword in ["è¿˜æœ‰", "å¦å¤–", "ç»§ç»­", "è¿›ä¸€æ­¥"]):
            return "DOCUMENT_QUERY"
        
        return self._pattern_based_analysis(question)

    def _document_aware_analysis(self, question: str, document_info: dict = None) -> str:
        """æ–‡æ¡£æ„ŸçŸ¥åˆ†æ"""
        if not document_info:
            return self._pattern_based_analysis(question)
        
        doc_count = document_info.get("count", 0)
        
        # å¤šæ–‡æ¡£åœºæ™¯ä¼˜å…ˆè€ƒè™‘æ–‡æ¡£æŸ¥è¯¢
        if doc_count > 1:
            return "DOCUMENT_QUERY"
        
        # å•æ–‡æ¡£åœºæ™¯ï¼Œæ£€æŸ¥é—®é¢˜ä¸­çš„æ–‡æ¡£å¼•ç”¨
        if any(keyword in question.lower() for keyword in ["è¿™ä¸ª", "è¯¥", "æ­¤"]):
            return "DOCUMENT_QUERY"
        
        return self._pattern_based_analysis(question)

    def _make_final_decision(self, base_intent: str, context_intent: str, 
                           doc_intent: str, has_documents: bool) -> str:
        """æœ€ç»ˆæ„å›¾å†³ç­–"""
        
        # æƒé‡æŠ•ç¥¨æœºåˆ¶
        intent_votes = defaultdict(float)
        intent_votes[base_intent] += 0.4
        intent_votes[context_intent] += 0.3
        intent_votes[doc_intent] += 0.3
        
        # å¦‚æœæœ‰æ–‡æ¡£ä½†æŠ•ç¥¨ç»“æœæ˜¯é€šç”¨æŸ¥è¯¢ï¼Œè°ƒæ•´æƒé‡
        if has_documents and max(intent_votes.keys(), key=intent_votes.get) == "GENERAL_QUERY":
            if intent_votes["DOCUMENT_QUERY"] > 0.2:
                return "DOCUMENT_QUERY"
        
        # è¿”å›å¾—ç¥¨æœ€é«˜çš„æ„å›¾
        return max(intent_votes.keys(), key=intent_votes.get)

class AIManager:
    def __init__(self, enable_memory: bool = True, enable_enhanced_intent: bool = True):
        # é…ç½®Gemini
        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        
        # ç”Ÿæˆé…ç½®
        self.generation_config = genai.types.GenerationConfig(
            temperature=0.1,
            top_p=0.95,
            top_k=32,
            max_output_tokens=8192,
            response_mime_type="text/plain",
        )
        
        # å®‰å…¨è®¾ç½®
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            }
        ]
        
        # é›†æˆå¢å¼ºåŠŸèƒ½
        self.enable_memory = enable_memory
        self.enable_enhanced_intent = enable_enhanced_intent
        
        # åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨
        if enable_memory:
            self.memory_manager = AdvancedMemoryManager()
            logger.info(f"[AI_INIT] è®°å¿†ç®¡ç†å·²å¯ç”¨")
        else:
            self.memory_manager = None
            logger.info(f"[AI_INIT] è®°å¿†ç®¡ç†å·²ç¦ç”¨")
        
        # åˆå§‹åŒ–å¢å¼ºæ„å›¾åˆ†æå™¨
        if enable_enhanced_intent:
            intent_config = IntentAnalysisConfig(
                use_advanced_analysis=True,
                enable_context_awareness=True,
                enable_document_awareness=True
            )
            self.intent_analyzer = EnhancedIntentAnalyzer(intent_config)
            logger.info(f"[AI_INIT] å¢å¼ºæ„å›¾åˆ†æå·²å¯ç”¨")
        else:
            self.intent_analyzer = None
            logger.info(f"[AI_INIT] ä½¿ç”¨ç®€åŒ–æ„å›¾åˆ†æ")
        
        logger.info(f"[AI_MANAGER] AIManageråˆå§‹åŒ–å®Œæˆ - è®°å¿†: {enable_memory}, å¢å¼ºæ„å›¾: {enable_enhanced_intent}")

    async def get_response(self, question: str, context: Union[str, Dict, List[Dict]], history: str = None, 
                          user_id: str = None, session_id: str = None, document_id: str = None) -> dict:
        """è·å–AIå›ç­” - é›†æˆè®°å¿†ç®¡ç†"""
        try:
            logger.info(f"[AI_RESPONSE] å¼€å§‹ç”Ÿæˆå›ç­” - é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦")
            
            # è®°å¿†å¢å¼º
            if self.memory_manager and user_id:
                # è·å–ç›¸å…³è®°å¿†
                relevant_memories = self.memory_manager.get_relevant_memories(
                    query=question,
                    user_id=user_id,
                    session_id=session_id,
                    limit=3
                )
                
                if relevant_memories:
                    memory_context = "\n".join([m.content for m in relevant_memories])
                    logger.debug(f"[AI_MEMORY] ä½¿ç”¨ {len(relevant_memories)} æ¡ç›¸å…³è®°å¿†")
                    
                    # å°†è®°å¿†èå…¥å†å²ä¸Šä¸‹æ–‡
                    if history:
                        history = f"ç›¸å…³è®°å¿†ï¼š{memory_context}\n\nå†å²å¯¹è¯ï¼š{history}"
                    else:
                        history = f"ç›¸å…³è®°å¿†ï¼š{memory_context}"
            
            # æ™ºèƒ½æ„å›¾è¯†åˆ«
            intent = "DOCUMENT_QUERY"  # é»˜è®¤å€¼
            if self.intent_analyzer:
                document_info = self._extract_document_info(context)
                intent = await self.intent_analyzer.analyze_intent(
                    question=question,
                    has_documents=bool(context),
                    history=history,
                    document_info=document_info
                )
            else:
                # ä½¿ç”¨ç®€åŒ–æ„å›¾åˆ†æ
                intent = await self.identify_intent(question, context, bool(context), history)
            
            logger.info(f"[AI_INTENT] è¯†åˆ«æ„å›¾: {intent}")
            
            # æ ¹æ®æ„å›¾é€‰æ‹©ç”Ÿæˆç­–ç•¥
            if intent == "GENERAL_QUERY":
                logger.info(f"[AI_MODE_GENERAL] ä½¿ç”¨é€šç”¨çŸ¥è¯†æ¨¡å¼")
                prompt = f"""ä½ æ˜¯CargoPPTçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚

å†å²å¯¹è¯ï¼š{history or 'æ— '}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æ ¹æ®ä½ çš„çŸ¥è¯†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œä¸è¦æåŠä»»ä½•æ–‡æ¡£ã€‚"""
                
            elif intent == "ANALYSIS_QUERY":
                logger.info(f"[AI_MODE_ANALYSIS] ä½¿ç”¨åˆ†ææŸ¥è¯¢æ¨¡å¼")
                # ä¸ºåˆ†ææŸ¥è¯¢æ„å»ºä¸“é—¨çš„prompt
                base_prompt = self._build_prompt(question, context, history)
                
                # æ™ºèƒ½åˆ†ææç¤º - ç»“åˆæ–‡æ¡£å†…å®¹å’Œå¤–éƒ¨çŸ¥è¯†
                analysis_instruction = """

## ğŸ“‹ åˆ†ææŒ‡å¯¼

**ä»»åŠ¡**: åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œç»“åˆä½ çš„çŸ¥è¯†åº“ï¼Œè¿›è¡Œæ·±åº¦åˆ†æå’Œæ¨ç†ã€‚

**åˆ†æç­–ç•¥**:
1. ğŸ“– **æ–‡æ¡£å†…å®¹æå–**: é¦–å…ˆæ€»ç»“æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯ã€æ¦‚å¿µå’Œåº”ç”¨æ¡ˆä¾‹
2. ğŸ§  **çŸ¥è¯†åº“æ‰©å±•**: åŸºäºæ–‡æ¡£å†…å®¹ï¼Œè°ƒç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œæ‹“å±•å’Œè¡¥å……
3. ğŸ”— **æ™ºèƒ½æ¨ç†**: å°†æ–‡æ¡£å†…å®¹ä¸å¤–éƒ¨çŸ¥è¯†ç»“åˆï¼Œè¿›è¡Œé€»è¾‘æ¨ç†å’Œåˆ†æ
4. ğŸ¯ **å…·ä½“åº”ç”¨**: é’ˆå¯¹ç”¨æˆ·æåŠçš„ç‰¹å®šé¢†åŸŸæˆ–åº”ç”¨åœºæ™¯ï¼Œæä¾›å…·ä½“çš„åˆ†æå’Œå»ºè®®

**å›ç­”è¦æ±‚**:
- âœ… é¦–å…ˆåŸºäºæ–‡æ¡£å†…å®¹å›ç­”
- âœ… ç„¶åç»“åˆç›¸å…³é¢†åŸŸçŸ¥è¯†è¿›è¡Œæ‰©å±•
- âœ… æä¾›å…·ä½“çš„åº”ç”¨åœºæ™¯å’Œå®æ–½å»ºè®®
- âœ… ä½¿ç”¨ç»“æ„åŒ–æ ¼å¼ï¼ˆæ ‡é¢˜ã€è¦ç‚¹ã€è¡¨æ ¼ç­‰ï¼‰
- âœ… å½“æ–‡æ¡£å†…å®¹ä¸è¶³æ—¶ï¼Œæ˜ç¡®è¯´æ˜å¹¶åŸºäºåˆç†æ¨ç†è¡¥å……

**ç‰¹åˆ«æ³¨æ„**: å¦‚æœç”¨æˆ·è¯¢é—®ç‰¹å®šè¡Œä¸šåº”ç”¨ï¼ˆå¦‚ç‰©æµã€é‡‘èã€åŒ»ç–—ç­‰ï¼‰ï¼Œå³ä½¿æ–‡æ¡£ä¸­æ²¡æœ‰ç›´æ¥æåŠï¼Œä¹Ÿè¦åŸºäºæ–‡æ¡£ä¸­çš„é€šç”¨æ¦‚å¿µå’Œä½ çš„ä¸“ä¸šçŸ¥è¯†ï¼Œæ¨ç†å‡ºåœ¨è¯¥è¡Œä¸šçš„å¯èƒ½åº”ç”¨ã€‚"""
                
                prompt = base_prompt + analysis_instruction
            
            elif intent in ["COMPARISON_QUERY", "SUMMARY_QUERY"]:
                logger.info(f"[AI_MODE_SPECIAL] ä½¿ç”¨ç‰¹æ®ŠæŸ¥è¯¢æ¨¡å¼: {intent}")
                prompt = self._build_prompt(question, context, history)
                
                if intent == "COMPARISON_QUERY":
                    prompt += "\nè¯·æä¾›è¯¦ç»†çš„æ¯”è¾ƒåˆ†æï¼ŒåŒ…æ‹¬ç›¸ä¼¼ç‚¹ã€ä¸åŒç‚¹å’Œä¼˜ç¼ºç‚¹ã€‚"
                else:  # SUMMARY_QUERY
                    prompt += "\nè¯·æä¾›ç»“æ„åŒ–çš„æ€»ç»“ï¼ŒåŒ…æ‹¬å…³é”®è¦ç‚¹ã€é‡è¦ç»“è®ºå’Œå®é™…æ„ä¹‰ã€‚"
            
            else:  # DOCUMENT_QUERY æˆ–é»˜è®¤
                logger.info(f"[AI_MODE_DOCUMENT] ä½¿ç”¨æ–‡æ¡£æŸ¥è¯¢æ¨¡å¼")
                prompt = self._build_prompt(question, context, history)
            
            logger.debug(f"[AI_PROMPT] æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # ç”Ÿæˆå›ç­”
            logger.info(f"[AI_GENERATE_START] å¼€å§‹è°ƒç”¨Gemini APIç”Ÿæˆå›ç­”")
            response = self.model.generate_content(
                prompt,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings
            )
            
            # æ£€æŸ¥å“åº”
            if not response or not response.text:
                logger.error(f"[AI_GENERATE_EMPTY] Geminiè¿”å›ç©ºå“åº”")
                return {
                    "answer": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›ç­”ã€‚è¯·é‡æ–°å°è¯•æ‚¨çš„é—®é¢˜ã€‚",
                    "sources": [],
                    "confidence": 0.0,
                    "reply": [{"type": "markdown", "content": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›ç­”ã€‚è¯·é‡æ–°å°è¯•æ‚¨çš„é—®é¢˜ã€‚"}]
                }
            
            answer = response.text.strip()
            logger.info(f"[AI_GENERATE_SUCCESS] å›ç­”ç”ŸæˆæˆåŠŸ - é•¿åº¦: {len(answer)} å­—ç¬¦")
            
            # è®°å¿†å­˜å‚¨
            if self.memory_manager and user_id:
                # å­˜å‚¨é—®é¢˜
                self.memory_manager.add_memory(
                    content=f"ç”¨æˆ·é—®é¢˜: {question}",
                    context_type="conversation",
                    importance=0.6,
                    user_id=user_id,
                    session_id=session_id,
                    document_id=document_id
                )
                
                # å­˜å‚¨é‡è¦ç­”æ¡ˆ
                if len(answer) > 100:  # åªå­˜å‚¨è¾ƒé•¿çš„ç­”æ¡ˆ
                    self.memory_manager.add_memory(
                        content=f"AIå›ç­”: {answer[:500]}...",  # æˆªå–å‰500å­—ç¬¦
                        context_type="conversation",
                        importance=0.7,
                        user_id=user_id,
                        session_id=session_id,
                        document_id=document_id
                    )
                
                logger.debug(f"[AI_MEMORY_STORE] å·²å­˜å‚¨å¯¹è¯è®°å¿†")
            
            # æå–æ¥æºä¿¡æ¯
            sources = []
            confidence = 0.8
            
            if isinstance(context, dict):
                sources = context.get("chunks", [])[:5]
                confidence = 0.9
            elif isinstance(context, list):
                sources = context[:5]
                confidence = 0.9
            
            # è§£æå›ç­”ä¸ºå¯Œæ–‡æœ¬æ ¼å¼
            reply = self._parse_response_content(answer)
            
            return {
                "answer": answer,
                "sources": sources,
                "confidence": confidence,
                "reply": reply,
                "intent": intent,  # è¿”å›è¯†åˆ«çš„æ„å›¾
                "memory_used": len(relevant_memories) if self.memory_manager and user_id and 'relevant_memories' in locals() else 0
            }
            
        except Exception as e:
            logger.error(f"[AI_RESPONSE_ERROR] ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}")
            import traceback
            logger.debug(f"[AI_RESPONSE_TRACEBACK] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            
            return {
                "answer": f"å¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "reply": [{"type": "markdown", "content": f"å¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}],
                "intent": "ERROR",
                "memory_used": 0
            }

    def _extract_document_info(self, context) -> dict:
        """ä»ä¸Šä¸‹æ–‡ä¸­æå–æ–‡æ¡£ä¿¡æ¯"""
        if not context:
            return {"count": 0, "names": []}
        
        if isinstance(context, dict):
            if "documents" in context:
                # å¤šæ–‡æ¡£æ ¼å¼
                docs = context["documents"]
                return {
                    "count": len(docs),
                    "names": [doc.get("document_name", "æœªçŸ¥æ–‡æ¡£") for doc in docs]
                }
            elif "document_name" in context:
                # å•æ–‡æ¡£æ ¼å¼
                return {
                    "count": 1,
                    "names": [context["document_name"]]
                }
        
        return {"count": 1, "names": ["æ–‡æ¡£"]}

    async def chat_without_context(self, message: str, history: List[Dict[str, str]] = None, 
                                 user_id: str = None, session_id: str = None) -> dict:
        """æ— ä¸Šä¸‹æ–‡èŠå¤© - é›†æˆè®°å¿†ç®¡ç†"""
        try:
            logger.info(f"[AI_CHAT] å¼€å§‹æ— ä¸Šä¸‹æ–‡èŠå¤© - æ¶ˆæ¯é•¿åº¦: {len(message)} å­—ç¬¦")
            
            # è®°å¿†å¢å¼º
            memory_context = ""
            if self.memory_manager and user_id:
                relevant_memories = self.memory_manager.get_relevant_memories(
                    query=message,
                    user_id=user_id,
                    session_id=session_id,
                    limit=3
                )
                
                if relevant_memories:
                    memory_context = "\n".join([f"- {m.content}" for m in relevant_memories])
                    logger.debug(f"[AI_CHAT_MEMORY] ä½¿ç”¨ {len(relevant_memories)} æ¡ç›¸å…³è®°å¿†")
            
            # æ„å»ºæç¤ºè¯
            history_text = ""
            if history:
                history_items = []
                for msg in history[-8:]:  # æœ€è¿‘8è½®å¯¹è¯
                    role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
                    history_items.append(f"{role}: {msg['content']}")
                history_text = "\n".join(history_items)
            
            prompt = f"""ä½ æ˜¯CargoPPTçš„AIåŠ©æ‰‹ï¼Œä¸€ä¸ªä¸“ä¸šã€å‹å¥½ä¸”æœ‰å¸®åŠ©çš„AIã€‚

{f"ç›¸å…³è®°å¿†ï¼š{memory_context}" if memory_context else ""}

{f"å¯¹è¯å†å²ï¼š{history_text}" if history_text else ""}

ç”¨æˆ·é—®é¢˜ï¼š{message}

è¯·æä¾›æœ‰å¸®åŠ©çš„å›ç­”ã€‚å¦‚æœé—®é¢˜æ¶‰åŠä¸“ä¸šçŸ¥è¯†ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†ç»™å‡ºå‡†ç¡®çš„ä¿¡æ¯ã€‚"""
            
            # ç”Ÿæˆå›ç­”
            response = self.model.generate_content( 
                prompt,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings
            )
            
            if not response or not response.text:
                logger.error(f"[AI_CHAT_EMPTY] Geminiè¿”å›ç©ºå“åº”")
                return {
                    "answer": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ã€‚è¯·é‡æ–°è¡¨è¿°ã€‚",
                            "sources": [],
                    "confidence": 0.0,
                    "reply": [{"type": "markdown", "content": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ã€‚è¯·é‡æ–°è¡¨è¿°ã€‚"}]
                }
            
            answer = response.text.strip()
            logger.info(f"[AI_CHAT_SUCCESS] èŠå¤©å›ç­”ç”ŸæˆæˆåŠŸ - é•¿åº¦: {len(answer)} å­—ç¬¦")
            
            # è®°å¿†å­˜å‚¨
            if self.memory_manager and user_id:
                # å­˜å‚¨é—®é¢˜
                self.memory_manager.add_memory(
                    content=f"ç”¨æˆ·é—®é¢˜: {message}",
                    context_type="conversation",
                    importance=0.5,
                    user_id=user_id,
                    session_id=session_id
                )
                
                # å­˜å‚¨ç­”æ¡ˆ
                if len(answer) > 50:
                    self.memory_manager.add_memory(
                        content=f"AIå›ç­”: {answer[:300]}...",
                        context_type="conversation",
                        importance=0.6,
                        user_id=user_id,
                        session_id=session_id
                    )
            
            # è§£æå›ç­”
            reply = self._parse_response_content(answer)
            
            return {
                "answer": answer,
                "sources": [],
                "confidence": 0.7,
                "reply": reply,
                "memory_used": len(relevant_memories) if self.memory_manager and user_id and 'relevant_memories' in locals() else 0
            }
            
        except Exception as e:
            logger.error(f"[AI_CHAT_ERROR] æ— ä¸Šä¸‹æ–‡èŠå¤©å¤±è´¥: {str(e)}")
            return {
                "answer": f"èŠå¤©æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "reply": [{"type": "markdown", "content": f"èŠå¤©æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}],
                "memory_used": 0
            }

    async def get_multi_document_response(self, question: str, document_contexts: List[Dict], history: str = None) -> dict:
        """å¤„ç†å¤šæ–‡æ¡£æŸ¥è¯¢ï¼Œæ•´åˆå¤šä¸ªæ–‡æ¡£çš„ç›¸å…³å†…å®¹"""
        logger.info(f"[MULTI_DOC] å¼€å§‹å¤šæ–‡æ¡£æŸ¥è¯¢ - é—®é¢˜: {question[:50]}..., æ–‡æ¡£æ•°é‡: {len(document_contexts)}")
        
        try:
            # æ•´åˆæ‰€æœ‰æ–‡æ¡£çš„ä¸Šä¸‹æ–‡
            combined_context = ""
            sources = []
            
            for i, doc_context in enumerate(document_contexts):
                if doc_context and doc_context.get("context"):
                    combined_context += f"\n\n=== æ–‡æ¡£ {i+1} ===\n{doc_context['context']}"
                    if doc_context.get("sources"):
                        sources.extend(doc_context["sources"])
            
            logger.debug(f"[MULTI_DOC_CONTEXT] åˆå¹¶ä¸Šä¸‹æ–‡é•¿åº¦: {len(combined_context)} å­—ç¬¦, æ¥æºæ•°é‡: {len(sources)}")
            
            # ä½¿ç”¨åˆå¹¶çš„ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”
            response = await self.get_response(question, combined_context, history)
            
            # æ›´æ–°æ¥æºä¿¡æ¯
            if response and isinstance(response, dict):
                response["sources"] = sources[:10]  # é™åˆ¶æ¥æºæ•°é‡
                logger.info(f"[MULTI_DOC_SUCCESS] å¤šæ–‡æ¡£æŸ¥è¯¢æˆåŠŸ - å“åº”é•¿åº¦: {len(response.get('answer', ''))} å­—ç¬¦")
            
            return response
            
        except Exception as e:
            logger.error(f"[MULTI_DOC_ERROR] å¤šæ–‡æ¡£æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return {
                "answer": f"å¤šæ–‡æ¡£æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "reply": [{"type": "markdown", "content": f"å¤šæ–‡æ¡£æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}]
            }

    async def analyze_structured_data(self, query: str, structured_data: dict, paper_id: str = None, is_sampled: bool = False) -> dict:
        """åˆ†æç»“æ„åŒ–æ•°æ®ï¼ˆè¡¨æ ¼ã€å›¾è¡¨ç­‰ï¼‰"""
        logger.info(f"[STRUCTURED_ANALYSIS] å¼€å§‹ç»“æ„åŒ–æ•°æ®åˆ†æ - æŸ¥è¯¢: {query[:50]}..., æ•°æ®ç±»å‹: {type(structured_data)}")
        
        try:
            # æ„å»ºç»“æ„åŒ–æ•°æ®çš„æ–‡æœ¬æè¿°
            data_description = self._format_structured_data(structured_data)
            
            # æ„å»ºä¸“é—¨çš„æç¤ºè¯
            prompt = f"""åŸºäºä»¥ä¸‹ç»“æ„åŒ–æ•°æ®å›ç­”é—®é¢˜ï¼š

æ•°æ®å†…å®¹ï¼š
{data_description}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä¸Šè¿°æ•°æ®æä¾›å‡†ç¡®çš„åˆ†æå’Œå›ç­”ã€‚å¦‚æœæ•°æ®ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""

            logger.debug(f"[STRUCTURED_PROMPT] ç»“æ„åŒ–æ•°æ®æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # ç”Ÿæˆå›ç­”
            response = self.model.generate_content(
                prompt,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings
            )
            
            if response and response.text:
                logger.info(f"[STRUCTURED_SUCCESS] ç»“æ„åŒ–æ•°æ®åˆ†ææˆåŠŸ - å“åº”é•¿åº¦: {len(response.text)} å­—ç¬¦")
                
                # è§£æå“åº”å†…å®¹
                parsed_content = self._parse_response_content(response.text)
                
                return {
                    "answer": response.text,
                    "sources": [{
                        "line_number": 0,
                        "content": "ç»“æ„åŒ–æ•°æ®åˆ†æ",
                        "page": 1,
                        "start_pos": 0,
                        "end_pos": len(str(structured_data)),
                        "is_scanned": False,
                        "similarity": 1.0,
                        "document_id": paper_id,
                        "document_name": "ç»“æ„åŒ–æ•°æ®"
                    }] if paper_id else [],
                    "confidence": 0.8,
                    "reply": parsed_content
                }
            else:
                logger.warning(f"[STRUCTURED_NO_RESPONSE] ç»“æ„åŒ–æ•°æ®åˆ†ææ— å“åº”")
                return {
                    "answer": "æ— æ³•åˆ†ææä¾›çš„ç»“æ„åŒ–æ•°æ®",
                    "sources": [],
                    "confidence": 0.0,
                    "reply": [{"type": "markdown", "content": "æ— æ³•åˆ†ææä¾›çš„ç»“æ„åŒ–æ•°æ®"}]
                }
                
        except Exception as e:
            logger.error(f"[STRUCTURED_ERROR] ç»“æ„åŒ–æ•°æ®åˆ†æå¤±è´¥: {str(e)}")
            return {
                "answer": f"ç»“æ„åŒ–æ•°æ®åˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "reply": [{"type": "markdown", "content": f"ç»“æ„åŒ–æ•°æ®åˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}]
            }

    def _format_structured_data(self, data: dict) -> str:
        """æ ¼å¼åŒ–ç»“æ„åŒ–æ•°æ®ä¸ºæ–‡æœ¬æè¿°"""
        logger.debug(f"[FORMAT_STRUCTURED] å¼€å§‹æ ¼å¼åŒ–ç»“æ„åŒ–æ•°æ® - æ•°æ®é”®: {list(data.keys()) if isinstance(data, dict) else 'Not dict'}")
        
        try:
            if not isinstance(data, dict):
                return str(data)
            
            formatted_parts = []
            
            # å¤„ç†è¡¨æ ¼æ•°æ®
            if "tables" in data and data["tables"]:
                formatted_parts.append("=== è¡¨æ ¼æ•°æ® ===")
                for i, table in enumerate(data["tables"][:5]):  # é™åˆ¶è¡¨æ ¼æ•°é‡
                    formatted_parts.append(f"\nè¡¨æ ¼ {i+1}:")
                    if isinstance(table, dict):
                        if "headers" in table and table["headers"]:
                            formatted_parts.append(f"åˆ—æ ‡é¢˜: {', '.join(table['headers'])}")
                        if "rows" in table and table["rows"]:
                            formatted_parts.append(f"æ•°æ®è¡Œæ•°: {len(table['rows'])}")
                            # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
                            for j, row in enumerate(table["rows"][:3]):
                                formatted_parts.append(f"  è¡Œ{j+1}: {row}")
                    else:
                        formatted_parts.append(f"  {str(table)[:200]}...")
            
            # å¤„ç†å›¾è¡¨æ•°æ®
            if "charts" in data and data["charts"]:
                formatted_parts.append("\n=== å›¾è¡¨æ•°æ® ===")
                for i, chart in enumerate(data["charts"][:3]):  # é™åˆ¶å›¾è¡¨æ•°é‡
                    formatted_parts.append(f"\nå›¾è¡¨ {i+1}:")
                    if isinstance(chart, dict):
                        chart_type = chart.get("type", "æœªçŸ¥ç±»å‹")
                        formatted_parts.append(f"  ç±»å‹: {chart_type}")
                        if "data" in chart:
                            formatted_parts.append(f"  æ•°æ®ç‚¹æ•°: {len(chart['data']) if isinstance(chart['data'], list) else 'æœªçŸ¥'}")
                    else:
                        formatted_parts.append(f"  {str(chart)[:200]}...")
            
            # å¤„ç†å…¶ä»–æ•°æ®
            for key, value in data.items():
                if key not in ["tables", "charts"] and value:
                    formatted_parts.append(f"\n=== {key} ===")
                    if isinstance(value, (list, dict)):
                        formatted_parts.append(f"{str(value)[:500]}...")
                    else:
                        formatted_parts.append(str(value)[:500])
            
            result = "\n".join(formatted_parts)
            logger.debug(f"[FORMAT_STRUCTURED_SUCCESS] ç»“æ„åŒ–æ•°æ®æ ¼å¼åŒ–å®Œæˆ - è¾“å‡ºé•¿åº¦: {len(result)} å­—ç¬¦")
            return result
            
        except Exception as e:
            logger.error(f"[FORMAT_STRUCTURED_ERROR] æ ¼å¼åŒ–ç»“æ„åŒ–æ•°æ®å¤±è´¥: {str(e)}")
            return f"æ•°æ®æ ¼å¼åŒ–å¤±è´¥: {str(e)}"

    async def stream_response(self, question, context, history=None):
        """æ”¹è¿›çš„æµå¼å“åº”ï¼Œå‚è€ƒChatGPTå®ç°ä¼˜åŒ–ï¼Œå¢åŠ chunkæ ¡éªŒ"""
        try:
            logger.info(f"[STREAM_RESPONSE] å¼€å§‹æµå¼å“åº” - é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦")
            logger.debug(f"[STREAM_QUESTION] é—®é¢˜å†…å®¹: {question[:100]}{'...' if len(question) > 100 else ''}")
            
            # éªŒè¯é—®é¢˜ä¸ä¸ºç©º
            if not question or not question.strip():
                logger.error(f"[STREAM_EMPTY_QUESTION] é—®é¢˜å†…å®¹ä¸ºç©º")
                yield {"error": "é—®é¢˜å†…å®¹ä¸èƒ½ä¸ºç©º", "done": True}
                return
            
            # æ„å»ºæç¤º
            logger.info(f"[STREAM_PROMPT_BUILD] å¼€å§‹æ„å»ºæç¤ºè¯")
            prompt = self._build_prompt(question, context, history)
            logger.debug(f"[STREAM_PROMPT] æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # å°è¯•ä½¿ç”¨çœŸæ­£çš„æµå¼API
            try:
                # ä½¿ç”¨æµå¼ç”Ÿæˆé…ç½®
                stream_config = {
                    "temperature": 0.7,
                    "max_output_tokens": 2048,
                    "top_p": 0.8,
                    "top_k": 40
                }
                
                logger.info(f"[STREAM_NATIVE_START] å°è¯•ä½¿ç”¨åŸç”Ÿæµå¼API")
                # å°è¯•çœŸæ­£çš„æµå¼ç”Ÿæˆ
                response = self.model.generate_content(
                    prompt,
                    generation_config=stream_config,
                    safety_settings=self.safety_settings,
                    stream=True  # å¯ç”¨æµå¼
                )
                
                # å¤„ç†çœŸæ­£çš„æµå¼å“åº”ï¼Œå¢åŠ æ ¡éªŒ
                chunk_count = 0
                for chunk in response:
                    chunk_count += 1
                    if hasattr(chunk, 'text') and chunk.text:
                        # æ ¡éªŒchunkå†…å®¹
                        validated_chunk = self._validate_stream_chunk(chunk.text)
                        if validated_chunk:
                            logger.debug(f"[STREAM_NATIVE_CHUNK] åŸç”Ÿchunk #{chunk_count} - é•¿åº¦: {len(validated_chunk)} å­—ç¬¦")
                            yield {
                                "partial_response": validated_chunk,
                                "done": False,
                                "type": "content"
                            }
                
                # æµç»“æŸæ ‡è®° - è¿™é‡Œä¸æ˜¯æœ€ç»ˆç¡®è®¤ï¼Œåªæ˜¯å†…å®¹ç»“æŸ
                logger.info(f"[STREAM_NATIVE_COMPLETE] åŸç”Ÿæµå¼å®Œæˆ - æ€»chunks: {chunk_count}")
                yield {"partial_response": "", "done": True, "type": "end", "content_complete": True}
                return
                
            except Exception as stream_error:
                logger.warning(f"[STREAM_NATIVE_FAILED] åŸç”Ÿæµå¼å¤±è´¥ï¼Œé™çº§å¤„ç†: {str(stream_error)}")
                # é™çº§åˆ°æ¨¡æ‹Ÿæµå¼
                
            # é™çº§å¤„ç†ï¼šä½¿ç”¨åŒæ­¥æ–¹æ³•
            logger.info(f"[STREAM_FALLBACK_START] å¼€å§‹é™çº§æµå¼å¤„ç†")
            response = self.model.generate_content(
                prompt,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings
            )
            
            # æ™ºèƒ½åˆ†å—ï¼Œå‚è€ƒChatGPTçš„å®ç°
            if hasattr(response, 'text'):
                full_text = response.text
                logger.info(f"[STREAM_FALLBACK_SUCCESS] é™çº§å“åº”æˆåŠŸ - å“åº”é•¿åº¦: {len(full_text)} å­—ç¬¦")
                # æŒ‰è¯­ä¹‰åˆ†å—è€Œä¸æ˜¯å›ºå®šå­—ç¬¦æ•°
                chunks = self._smart_chunk_text(full_text)
                logger.info(f"[STREAM_CHUNKS] æ–‡æœ¬åˆ†å—å®Œæˆ - æ€»å—æ•°: {len(chunks)}")
                
                for i, chunk in enumerate(chunks):
                    # æ ¡éªŒæ¯ä¸ªchunk
                    validated_chunk = self._validate_stream_chunk(chunk)
                    if validated_chunk:
                        logger.debug(f"[STREAM_FALLBACK_CHUNK] é™çº§chunk #{i+1}/{len(chunks)} - é•¿åº¦: {len(validated_chunk)} å­—ç¬¦")
                        yield {
                            "partial_response": validated_chunk,
                            "done": i == len(chunks) - 1,
                            "type": "content",
                            "chunk_index": i
                        }
                        # å‡å°‘å»¶è¿Ÿï¼Œæé«˜å“åº”é€Ÿåº¦
                        await asyncio.sleep(0.005)  # 5mså»¶è¿Ÿï¼Œæ¯”åŸæ¥çš„10msæ›´å¿«
                
                logger.info(f"[STREAM_FALLBACK_COMPLETE] é™çº§æµå¼å®Œæˆ")
                # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯å†…å®¹å®Œæˆï¼Œä¸æ˜¯æœ€ç»ˆç¡®è®¤
                yield {"partial_response": "", "done": True, "type": "content_end", "content_complete": True}
            else:
                logger.error(f"[STREAM_NO_TEXT] æ— æ³•è·å–å“åº”å†…å®¹")
                yield {"error": "æ— æ³•è·å–å“åº”å†…å®¹", "done": True}
            
        except Exception as e:
            logger.error(f"[STREAM_ERROR] æµå¼ç”Ÿæˆé”™è¯¯: {str(e)}")
            import traceback
            logger.debug(f"[STREAM_TRACEBACK] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            
            # ç®€åŒ–çš„é™çº§å¤„ç†
            try:
                logger.info(f"[STREAM_SIMPLE_FALLBACK] å°è¯•ç®€å•é™çº§")
                fallback_response = self.model.generate_content(
                    f"è¯·ç®€æ´å›ç­”ï¼š{question}",
                    generation_config={"temperature": 0.7, "max_output_tokens": 512}
                )
                
                if fallback_response and fallback_response.text:
                    logger.info(f"[STREAM_SIMPLE_SUCCESS] ç®€å•é™çº§æˆåŠŸ - å“åº”é•¿åº¦: {len(fallback_response.text)} å­—ç¬¦")
                    # å¿«é€Ÿè¾“å‡ºé™çº§å“åº”
                    chunks = self._smart_chunk_text(fallback_response.text)
                    for i, chunk in enumerate(chunks):
                        validated_chunk = self._validate_stream_chunk(chunk)
                        if validated_chunk:
                            logger.debug(f"[STREAM_SIMPLE_CHUNK] ç®€å•é™çº§chunk #{i+1} - é•¿åº¦: {len(validated_chunk)} å­—ç¬¦")
                            yield {
                                "partial_response": validated_chunk,
                                "done": i == len(chunks) - 1,
                                "fallback_used": True,
                                "type": "content"
                            }
                            await asyncio.sleep(0.003)  # æ›´å¿«çš„é™çº§å“åº”
                
                logger.info(f"[STREAM_SIMPLE_FALLBACK_COMPLETE] ç®€å•é™çº§å®Œæˆ")
                yield {"partial_response": "", "done": True, "type": "content_end", "content_complete": True}
                return
            except Exception as fallback_error:
                logger.error(f"[STREAM_SIMPLE_FAILED] ç®€å•é™çº§ä¹Ÿå¤±è´¥: {str(fallback_error)}")
            
            yield {
                "error": "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•",
                "done": True,
                "type": "error"
            }

    def _validate_stream_chunk(self, chunk_text: str) -> str:
        """æ ¡éªŒæµå¼chunkçš„æ ¼å¼å’Œå†…å®¹ï¼Œå¢å¼ºå¼‚å¸¸å¤„ç†"""
        try:
            # åŸºæœ¬æ ¡éªŒï¼šç¡®ä¿æ˜¯å­—ç¬¦ä¸²
            if chunk_text is None:
                logger.debug(f"[CHUNK_VALIDATE] chunkä¸ºNoneï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
                return ""
            
            if not isinstance(chunk_text, str):
                logger.warning(f"[CHUNK_VALIDATE] chunkä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(chunk_text)}")
                try:
                    # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    if hasattr(chunk_text, 'text'):  # Gemini responseå¯¹è±¡
                        chunk_text = chunk_text.text
                        logger.debug(f"[CHUNK_CONVERT] ä»Geminiå¯¹è±¡æå–æ–‡æœ¬")
                    elif isinstance(chunk_text, (dict, list)):
                        # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œå°è¯•æå–æ–‡æœ¬å†…å®¹
                        chunk_text = self._extract_text_from_object(chunk_text)
                        logger.debug(f"[CHUNK_CONVERT] ä»å¤æ‚å¯¹è±¡æå–æ–‡æœ¬")
                    else:
                        chunk_text = str(chunk_text)
                        logger.debug(f"[CHUNK_CONVERT] å¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²")
                except Exception as convert_error:
                    logger.error(f"[CHUNK_CONVERT_ERROR] è½¬æ¢chunkä¸ºå­—ç¬¦ä¸²å¤±è´¥: {str(convert_error)}")
                    return ""
            
            # é•¿åº¦æ ¡éªŒï¼šé˜²æ­¢å¼‚å¸¸å¤§çš„chunk
            if len(chunk_text) > 50000:  # 50KBé™åˆ¶
                logger.warning(f"[CHUNK_LARGE] chunkè¿‡å¤§({len(chunk_text)}å­—ç¬¦)ï¼Œæˆªæ–­å¤„ç†")
                chunk_text = chunk_text[:50000] + "..."
            
            # å†…å®¹æ ¡éªŒï¼šç§»é™¤æ§åˆ¶å­—ç¬¦ä½†ä¿ç•™å¿…è¦çš„æ ¼å¼å­—ç¬¦
            try:
                # ç§»é™¤é™¤äº†æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ã€å›è½¦ç¬¦å¤–çš„æ§åˆ¶å­—ç¬¦
                cleaned_text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', chunk_text)
                
                # å¤„ç†ç‰¹æ®Šçš„Unicodeå­—ç¬¦
                cleaned_text = self._handle_unicode_characters(cleaned_text)
                
                logger.debug(f"[CHUNK_CLEAN] chunkæ¸…ç†å®Œæˆ - åŸé•¿åº¦: {len(chunk_text)}, æ¸…ç†å: {len(cleaned_text)}")
                
            except re.error as regex_error:
                logger.error(f"[CHUNK_REGEX_ERROR] æ­£åˆ™è¡¨è¾¾å¼å¤„ç†chunkå¤±è´¥: {str(regex_error)}")
                # é™çº§å¤„ç†ï¼šé€å­—ç¬¦è¿‡æ»¤
                cleaned_text = self._char_by_char_clean(chunk_text)
                logger.debug(f"[CHUNK_CHAR_CLEAN] ä½¿ç”¨é€å­—ç¬¦æ¸…ç†")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆå†…å®¹ï¼ˆä¸åªæ˜¯ç©ºç™½å­—ç¬¦ï¼‰
            if not cleaned_text.strip():
                logger.debug(f"[CHUNK_EMPTY] chunkæ¸…ç†åä¸ºç©º")
                return ""  # è¿”å›ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯None
            
            # éªŒè¯ç¼–ç å®Œæ•´æ€§
            try:
                # å°è¯•ç¼–ç å’Œè§£ç ï¼Œç¡®ä¿å­—ç¬¦ä¸²å®Œæ•´
                cleaned_text.encode('utf-8').decode('utf-8')
                logger.debug(f"[CHUNK_ENCODING] chunkç¼–ç éªŒè¯é€šè¿‡")
            except UnicodeError as unicode_error:
                logger.warning(f"[CHUNK_UNICODE_ERROR] Unicodeç¼–ç é”™è¯¯: {str(unicode_error)}")
                # ä¿®å¤ç¼–ç é—®é¢˜
                cleaned_text = self._fix_unicode_errors(cleaned_text)
                logger.debug(f"[CHUNK_UNICODE_FIXED] Unicodeé”™è¯¯å·²ä¿®å¤")
            
            logger.debug(f"[CHUNK_VALIDATE_SUCCESS] chunkæ ¡éªŒæˆåŠŸ - æœ€ç»ˆé•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
            return cleaned_text
            
        except Exception as validation_error:
            logger.error(f"[CHUNK_VALIDATE_ERROR] Chunkæ ¡éªŒå¤±è´¥: {str(validation_error)}")
            import traceback
            logger.debug(f"[CHUNK_VALIDATE_TRACEBACK] æ ¡éªŒé”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            
            # æœ€ç»ˆé™çº§å¤„ç†ï¼šè¿”å›å®‰å…¨çš„å­—ç¬¦ä¸²
            try:
                if chunk_text is not None:
                    # å°è¯•æœ€åŸºæœ¬çš„å­—ç¬¦ä¸²è½¬æ¢
                    safe_text = str(chunk_text)
                    # ç§»é™¤æ˜æ˜¾çš„é—®é¢˜å­—ç¬¦
                    safe_text = ''.join(char for char in safe_text if ord(char) >= 32 or char in '\n\t\r')
                    logger.debug(f"[CHUNK_SAFE_FALLBACK] ä½¿ç”¨å®‰å…¨é™çº§ - é•¿åº¦: {len(safe_text[:1000])}")
                    return safe_text[:1000]  # é™åˆ¶é•¿åº¦
                else:
                    logger.debug(f"[CHUNK_NULL_FALLBACK] chunkä¸ºnullï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
                    return ""
            except:
                logger.error(f"[CHUNK_FINAL_FALLBACK] æœ€ç»ˆé™çº§ä¹Ÿå¤±è´¥")
                return ""

    def _extract_text_from_object(self, obj):
        """ä»å¤æ‚å¯¹è±¡ä¸­æå–æ–‡æœ¬å†…å®¹"""
        try:
            logger.debug(f"[TEXT_EXTRACT] å¼€å§‹ä»å¯¹è±¡æå–æ–‡æœ¬ - ç±»å‹: {type(obj)}")
            if isinstance(obj, dict):
                # å°è¯•å¸¸è§çš„æ–‡æœ¬å­—æ®µ
                for key in ['text', 'content', 'message', 'data', 'response']:
                    if key in obj:
                        logger.debug(f"[TEXT_EXTRACT] ä»å­—å…¸å­—æ®µ '{key}' æå–æ–‡æœ¬")
                        return str(obj[key])
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›æ•´ä¸ªå¯¹è±¡çš„å­—ç¬¦ä¸²è¡¨ç¤º
                logger.debug(f"[TEXT_EXTRACT] æœªæ‰¾åˆ°æ–‡æœ¬å­—æ®µï¼Œè¿”å›æ•´ä¸ªå­—å…¸çš„å­—ç¬¦ä¸²è¡¨ç¤º")
                return str(obj)
            elif isinstance(obj, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå°è¯•è¿æ¥æ‰€æœ‰å­—ç¬¦ä¸²å…ƒç´ 
                text_parts = []
                for item in obj:
                    if isinstance(item, str):
                        text_parts.append(item)
                    elif isinstance(item, dict) and 'text' in item:
                        text_parts.append(str(item['text']))
                result = ' '.join(text_parts) if text_parts else str(obj)
                logger.debug(f"[TEXT_EXTRACT] ä»åˆ—è¡¨æå–æ–‡æœ¬ - æ‰¾åˆ° {len(text_parts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
                return result
            else:
                logger.debug(f"[TEXT_EXTRACT] ç›´æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²")
                return str(obj)
        except Exception as e:
            logger.error(f"[TEXT_EXTRACT_ERROR] æå–æ–‡æœ¬å¤±è´¥: {str(e)}")
            return str(obj)

    def _handle_unicode_characters(self, text: str) -> str:
        """å¤„ç†ç‰¹æ®Šçš„Unicodeå­—ç¬¦"""
        try:
            original_length = len(text)
            # å¤„ç†å¸¸è§çš„Unicodeé—®é¢˜
            # æ›¿æ¢é›¶å®½å­—ç¬¦
            text = re.sub(r'[\u200b-\u200f\u2028-\u202f\u205f-\u206f\ufeff]', '', text)
            
            # è§„èŒƒåŒ–Unicode
            import unicodedata
            text = unicodedata.normalize('NFKC', text)
            
            logger.debug(f"[UNICODE_HANDLE] Unicodeå¤„ç†å®Œæˆ - åŸé•¿åº¦: {original_length}, å¤„ç†å: {len(text)}")
            return text
        except Exception as e:
            logger.error(f"[UNICODE_HANDLE_ERROR] Unicodeå¤„ç†å¤±è´¥: {str(e)}")
            return text

    def _char_by_char_clean(self, text: str) -> str:
        """é€å­—ç¬¦æ¸…ç†æ–‡æœ¬ï¼ˆé™çº§æ–¹æ³•ï¼‰"""
        try:
            original_length = len(text)
            cleaned_chars = []
            for char in text:
                try:
                    # ä¿ç•™å¯æ‰“å°å­—ç¬¦å’Œå¿…è¦çš„ç©ºç™½å­—ç¬¦
                    if char.isprintable() or char in '\n\t\r ':
                        cleaned_chars.append(char)
                except:
                    # å¦‚æœå­—ç¬¦æ£€æŸ¥å¤±è´¥ï¼Œè·³è¿‡è¯¥å­—ç¬¦
                    continue
            result = ''.join(cleaned_chars)
            logger.debug(f"[CHAR_CLEAN] é€å­—ç¬¦æ¸…ç†å®Œæˆ - åŸé•¿åº¦: {original_length}, æ¸…ç†å: {len(result)}")
            return result
        except Exception as e:
            logger.error(f"[CHAR_CLEAN_ERROR] é€å­—ç¬¦æ¸…ç†å¤±è´¥: {str(e)}")
            return text

    def _fix_unicode_errors(self, text: str) -> str:
        """ä¿®å¤Unicodeç¼–ç é”™è¯¯"""
        try:
            original_length = len(text)
            # å°è¯•ä¸åŒçš„ç¼–ç ä¿®å¤ç­–ç•¥
            # ç­–ç•¥1: ä½¿ç”¨errors='ignore'
            fixed_text = text.encode('utf-8', errors='ignore').decode('utf-8')
            
            # ç­–ç•¥2: å¦‚æœè¿˜æœ‰é—®é¢˜ï¼Œä½¿ç”¨errors='replace'
            if not fixed_text:
                fixed_text = text.encode('utf-8', errors='replace').decode('utf-8')
                logger.debug(f"[UNICODE_FIX] ä½¿ç”¨replaceç­–ç•¥ä¿®å¤")
            else:
                logger.debug(f"[UNICODE_FIX] ä½¿ç”¨ignoreç­–ç•¥ä¿®å¤")
            
            logger.debug(f"[UNICODE_FIX] Unicodeä¿®å¤å®Œæˆ - åŸé•¿åº¦: {original_length}, ä¿®å¤å: {len(fixed_text)}")
            return fixed_text
        except Exception as e:
            logger.error(f"[UNICODE_FIX_ERROR] Unicodeä¿®å¤å¤±è´¥: {str(e)}")
            # æœ€åçš„é™çº§ï¼šåªä¿ç•™ASCIIå­—ç¬¦
            ascii_text = ''.join(char for char in text if ord(char) < 128)
            logger.debug(f"[UNICODE_FIX_ASCII] é™çº§ä¸ºASCIIå­—ç¬¦ - é•¿åº¦: {len(ascii_text)}")
            return ascii_text

    def _smart_chunk_text(self, text: str, target_chunk_size: int = 8) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å—ï¼Œå‚è€ƒChatGPTçš„è¯­ä¹‰åˆ†å—ç­–ç•¥"""
        if not text:
            return [""]
        
        # ä¼˜å…ˆæŒ‰å¥å­åˆ†å‰²
        sentences = []
        current_sentence = ""
        
        for char in text:
            current_sentence += char
            # å¥å­ç»“æŸæ ‡è®°
            if char in '.!?ã€‚ï¼ï¼Ÿ\n':
                if current_sentence.strip():
                    sentences.append(current_sentence.strip())
                current_sentence = ""
        
        # æ·»åŠ å‰©ä½™å†…å®¹
        if current_sentence.strip():
            sentences.append(current_sentence.strip())
        
        # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„å¥å­åˆ†å‰²ï¼ŒæŒ‰è¯åˆ†å‰²
        if len(sentences) <= 1:
            words = text.split()
            chunks = []
            current_chunk = ""
            
            for word in words:
                if len(current_chunk) + len(word) + 1 <= target_chunk_size * 2:
                    current_chunk += (" " if current_chunk else "") + word
                else:
                    if current_chunk:
                        chunks.append(current_chunk)
                    current_chunk = word
            
            if current_chunk:
                chunks.append(current_chunk)
            
            return chunks if chunks else [text]
        
        # æŒ‰å¥å­ç»„åˆæˆåˆé€‚å¤§å°çš„å—
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) + 1 <= target_chunk_size * 3:
                current_chunk += (" " if current_chunk else "") + sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks if chunks else [text]

    def _build_prompt(self, question, context, history=None, is_general_chat=False):
        """æ„å»ºæç¤ºè¯ï¼Œç”¨äºç»Ÿä¸€ä¸åŒæ–¹æ³•çš„æç¤ºè¯æ ¼å¼"""
        
        if is_general_chat:
            # é€šç”¨èŠå¤©æ¨¡å¼æç¤ºè¯
            history_text = history or "æ— "
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”å„ç§é—®é¢˜ã€‚æ— è®ºæ˜¯ä¸€èˆ¬çŸ¥è¯†ã€å½“å‰äº‹ä»¶ã€ç§‘å­¦æ¢ç´¢ã€æ–‡å­¦è‰ºæœ¯ï¼Œè¿˜æ˜¯æŠ€æœ¯é—®é¢˜ï¼Œä½ éƒ½å¯ä»¥æä¾›ä¸°å¯Œçš„ä¿¡æ¯å’Œæ·±å…¥çš„è§è§£ã€‚

å†å²å¯¹è¯:
{history_text}

ç”¨æˆ·é—®é¢˜: {question}

è¯·ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸éœ€è¦å‚è€ƒä»»ä½•æ–‡æ¡£èµ„æ–™ã€‚æ ¹æ®é—®é¢˜æ€§è´¨çµæ´»è°ƒæ•´å›ç­”é£æ ¼å’Œæ·±åº¦ã€‚"""
        else:
            # æ„å»ºåŸºæœ¬æç¤º - æ–‡æ¡£åˆ†ææ¨¡å¼
            prompt = "ä½ æ˜¯CargoPPTçš„AIåŠ©æ‰‹ï¼Œä¸“é—¨ç”¨äºæ–‡æ¡£åˆ†æå’Œé—®ç­”ï¼Œè¯·æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚\n\n"
            
            # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
            if isinstance(context, dict) and "text" in context:
                # å¦‚æœæ˜¯æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡å¯¹è±¡
                prompt += f"æ–‡æ¡£å†…å®¹ï¼š\n{context['text']}\n\n"
            elif isinstance(context, list) and len(context) > 0:
                # å¦‚æœæ˜¯ä¸Šä¸‹æ–‡åˆ—è¡¨
                if all(isinstance(item, dict) for item in context):
                    # å¤šæ–‡æ¡£æƒ…å†µ
                    for doc in context:
                        if "text" in doc:
                            doc_info = f"æ–‡æ¡£ï¼š{doc.get('document_name', 'æœªçŸ¥æ–‡æ¡£')}\n"
                            prompt += f"{doc_info}{doc['text']}\n\n"
                else:
                    # çº¯æ–‡æœ¬åˆ—è¡¨
                    prompt += f"æ–‡æ¡£å†…å®¹ï¼š\n{' '.join(context)}\n\n"
            elif isinstance(context, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²
                prompt += f"æ–‡æ¡£å†…å®¹ï¼š\n{context}\n\n"
        
        # æ·»åŠ å†å²å¯¹è¯
        if history:
            if not is_general_chat:  # æ–‡æ¡£æ¨¡å¼ç‰¹æœ‰
                prompt += f"å†å²å¯¹è¯ï¼š\n{history}\n\n"
        
        # æ·»åŠ å½“å‰é—®é¢˜
        prompt += f"é—®é¢˜ï¼š{question}\n\n"
        
        if not is_general_chat:  # æ–‡æ¡£æ¨¡å¼ç‰¹æœ‰
            prompt += "è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æä¾›è¯¦ç»†å‡†ç¡®çš„å›ç­”ã€‚å›ç­”è¦ä¿æŒä¸¥è°¨ä¸”åŸºäºæ–‡æ¡£å†…å®¹ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚"
        
        return prompt
#0531 æ–°å¢æ„å›¾è¯†åˆ«æ–¹æ³•  
    async def identify_intent(self, question: str, context=None, has_documents=False, history=None, document_info=None):
        """ç®€åŒ–çš„æ„å›¾è¯†åˆ«ï¼Œåªè¿”å›GENERAL_QUERYæˆ–DOCUMENT_QUERY"""
        
        logger.info(f"[INTENT_IDENTIFY] å¼€å§‹æ„å›¾è¯†åˆ« - é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦, æœ‰æ–‡æ¡£: {has_documents}")
        
        try:
            # è°ƒç”¨ç®€åŒ–çš„æ„å›¾åˆ†æå™¨
            intent_result = await self.intent_analyzer.analyze_intent(
                question=question,
                has_documents=has_documents,
                history=history,
                document_info=document_info
            )
            
            logger.info(f"[INTENT_IDENTIFY_RESULT] æ„å›¾è¯†åˆ«å®Œæˆ - ç»“æœ: {intent_result}")
            return intent_result
            
        except Exception as e:
            logger.error(f"[INTENT_IDENTIFY_ERROR] æ„å›¾è¯†åˆ«å¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶çš„fallbacké€»è¾‘
            if not has_documents:
                return "GENERAL_QUERY"
            else:
                # æ£€æŸ¥ç®€å•çš„æ–‡æ¡£å…³é”®è¯
                doc_keywords = ["æ–‡æ¡£", "è¿™ç¯‡", "æ€»ç»“", "æ‘˜è¦", "åˆ†æ"]
                if any(keyword in question.lower() for keyword in doc_keywords):
                    return "DOCUMENT_QUERY"
                else:
                    return "GENERAL_QUERY"
#0531 æ–°å¢æ— ä¸Šä¸‹æ–‡æµå¼èŠå¤©æ–¹æ³•
    async def chat_without_context_stream(self, message: str, history: List[Dict[str, str]] = None):
        """æ— ä¸Šä¸‹æ–‡çš„æµå¼èŠå¤©ï¼Œç”¨äºé€šç”¨å¯¹è¯"""
        logger.info(f"[CHAT_STREAM] å¼€å§‹æ— ä¸Šä¸‹æ–‡æµå¼èŠå¤© - æ¶ˆæ¯é•¿åº¦: {len(message)} å­—ç¬¦")
        
        try:
            # éªŒè¯æ¶ˆæ¯ä¸ä¸ºç©º
            if not message or not message.strip():
                logger.error(f"[CHAT_EMPTY_MESSAGE] æ¶ˆæ¯å†…å®¹ä¸ºç©º")
                yield {"error": "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º", "done": True}
                return
            
            # æ„å»ºå¯¹è¯å†å²æ ¼å¼
            chat_history = []
            if history and isinstance(history, list):
                logger.info(f"[CHAT_HISTORY] å¤„ç†å†å²æ¶ˆæ¯ - æ•°é‡: {len(history)}")
                for i, msg in enumerate(history[-6:]):  # é™åˆ¶å†å²é•¿åº¦
                    if isinstance(msg, dict) and "role" in msg and "content" in msg and msg["content"].strip():
                        # è½¬æ¢ä¸ºGemini APIæ‰€éœ€çš„æ ¼å¼
                        role = "user" if msg["role"] == "user" else "model"
                        chat_history.append({
                            "role": role,
                            "parts": [{"text": msg["content"].strip()}]
                        })
                        logger.debug(f"[CHAT_HISTORY_ITEM] å†å²æ¶ˆæ¯ #{i+1} - è§’è‰²: {role}, é•¿åº¦: {len(msg['content'])} å­—ç¬¦")
                logger.info(f"[CHAT_HISTORY_BUILT] å†å²æ¶ˆæ¯æ„å»ºå®Œæˆ - æœ‰æ•ˆæ¶ˆæ¯: {len(chat_history)} æ¡")
            else:
                logger.info(f"[CHAT_NO_HISTORY] æ— å†å²æ¶ˆæ¯")
            
            # å°è¯•çœŸæ­£çš„æµå¼ç”Ÿæˆ
            try:
                stream_config = {
                    "temperature": 0.8,
                    "max_output_tokens": 1024,
                    "top_p": 0.9,
                    "top_k": 40
                }
                
                if not chat_history:
                    # æ— å†å²è®°å½•çš„ç‹¬ç«‹å¯¹è¯
                    logger.info(f"[CHAT_INDEPENDENT] ç‹¬ç«‹å¯¹è¯æ¨¡å¼")
                    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå¼ºå¤§çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”å„ç§é—®é¢˜ã€‚è¯·ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„ä¿¡æ¯ã€‚

ç”¨æˆ·é—®é¢˜: {message.strip()}"""
                    
                    logger.debug(f"[CHAT_PROMPT] ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)} å­—ç¬¦")
                    
                    response = self.model.generate_content(
                        system_prompt,
                        generation_config=stream_config,
                        safety_settings=self.safety_settings,
                        stream=True
                    )
                else:
                    # æœ‰å†å²è®°å½•çš„è¿ç»­å¯¹è¯
                    logger.info(f"[CHAT_CONTINUOUS] è¿ç»­å¯¹è¯æ¨¡å¼")
                    chat = self.model.start_chat(history=chat_history)
                    response = chat.send_message(
                        message.strip(),
                        generation_config=stream_config,
                        safety_settings=self.safety_settings,
                        stream=True
                    )
                
                # å¤„ç†çœŸæ­£çš„æµå¼å“åº”ï¼Œå¢åŠ æ ¡éªŒ
                logger.info(f"[CHAT_NATIVE_START] å¼€å§‹åŸç”Ÿæµå¼å¤„ç†")
                chunk_count = 0
                for chunk in response:
                    chunk_count += 1
                    if hasattr(chunk, 'text') and chunk.text:
                        # æ ¡éªŒchunkå†…å®¹
                        validated_chunk = self._validate_stream_chunk(chunk.text)
                        if validated_chunk:
                            logger.debug(f"[CHAT_NATIVE_CHUNK] åŸç”Ÿchunk #{chunk_count} - é•¿åº¦: {len(validated_chunk)} å­—ç¬¦")
                            yield {
                                "partial_response": validated_chunk,
                                "done": False,
                                "type": "content"
                            }
                
                logger.info(f"[CHAT_NATIVE_COMPLETE] åŸç”Ÿæµå¼å®Œæˆ - æ€»chunks: {chunk_count}")
                yield {"partial_response": "", "done": True, "type": "end"}
                return
                
            except Exception as stream_error:
                logger.warning(f"[CHAT_NATIVE_FAILED] åŸç”Ÿæµå¼å¤±è´¥ï¼Œé™çº§å¤„ç†: {str(stream_error)}")
            
            # é™çº§åˆ°æ¨¡æ‹Ÿæµå¼
            logger.info(f"[CHAT_FALLBACK_START] å¼€å§‹é™çº§æµå¼å¤„ç†")
            if not chat_history:
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå¼ºå¤§çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”å„ç§é—®é¢˜ï¼ŒåŒ…æ‹¬ä¸€èˆ¬çŸ¥è¯†é—®é¢˜ã€æ·±åº¦è§£æé—®é¢˜å’Œä¸“ä¸šé¢†åŸŸé—®é¢˜ã€‚
                
ä½ åº”è¯¥:
1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„ä¿¡æ¯
2. ä½¿ç”¨æ¸…æ™°ã€è‡ªç„¶çš„è¯­è¨€è¿›è¡Œäº¤æµ
3. åœ¨é€‚å½“æ—¶ä½¿ç”¨Markdownæ ¼å¼å¢å¼ºå¯è¯»æ€§
4. å½“æ¶‰åŠä¸“ä¸šçŸ¥è¯†æ—¶ï¼Œæä¾›æ·±å…¥çš„è§£æ
5. å½“ä¸ç¡®å®šç­”æ¡ˆæ—¶ï¼Œè¯šå®è¡¨æ˜

ç”¨æˆ·é—®é¢˜: {message.strip()}

è¯·ç›´æ¥å›ç­”ä¸Šè¿°é—®é¢˜ï¼Œä¸éœ€è¦æåŠæ–‡æ¡£ã€‚"""
                
                logger.debug(f"[CHAT_FALLBACK_PROMPT] é™çº§æç¤ºè¯é•¿åº¦: {len(system_prompt)} å­—ç¬¦")
                
                response = self.model.generate_content(
                    system_prompt,
                    generation_config=self.generation_config,
                    safety_settings=self.safety_settings
                )
            else:
                # ä½¿ç”¨å†å²å¯¹è¯å¼€å§‹èŠå¤©
                logger.info(f"[CHAT_FALLBACK_HISTORY] ä½¿ç”¨å†å²å¯¹è¯é™çº§å¤„ç†")
                chat = self.model.start_chat(history=chat_history)
                response = chat.send_message(
                    message.strip(),
                    generation_config=self.generation_config,
                    safety_settings=self.safety_settings
                )
            
            # ä½¿ç”¨æ™ºèƒ½åˆ†å—å¤„ç†å“åº”ï¼Œå¢åŠ æ ¡éªŒ
            if hasattr(response, 'text'):
                full_text = response.text
                logger.info(f"[CHAT_FALLBACK_SUCCESS] é™çº§å“åº”æˆåŠŸ - å“åº”é•¿åº¦: {len(full_text)} å­—ç¬¦")
                chunks = self._smart_chunk_text(full_text, target_chunk_size=10)  # é€šç”¨èŠå¤©å¯ä»¥ç¨å¤§ä¸€äº›
                logger.info(f"[CHAT_CHUNKS] æ–‡æœ¬åˆ†å—å®Œæˆ - æ€»å—æ•°: {len(chunks)}")
                
                for i, chunk in enumerate(chunks):
                    # æ ¡éªŒæ¯ä¸ªchunk
                    validated_chunk = self._validate_stream_chunk(chunk)
                    if validated_chunk:
                        logger.debug(f"[CHAT_FALLBACK_CHUNK] é™çº§chunk #{i+1}/{len(chunks)} - é•¿åº¦: {len(validated_chunk)} å­—ç¬¦")
                        yield {
                            "partial_response": validated_chunk,
                            "done": i == len(chunks) - 1,
                            "type": "content",
                            "chunk_index": i
                        }
                        await asyncio.sleep(0.008)  # é€šç”¨èŠå¤©ç¨æ…¢ä¸€ç‚¹ï¼Œæ›´è‡ªç„¶
                
                logger.info(f"[CHAT_FALLBACK_COMPLETE] é™çº§æµå¼å®Œæˆ")
            else:
                logger.error(f"[CHAT_NO_TEXT] æ— æ³•è·å–å“åº”å†…å®¹")
                yield {"error": "æ— æ³•è·å–å“åº”å†…å®¹", "done": True}
                
        except Exception as e:
            logger.error(f"[CHAT_ERROR] é€šç”¨èŠå¤©æµå¼ç”Ÿæˆé”™è¯¯: {str(e)}")
            import traceback
            logger.debug(f"[CHAT_TRACEBACK] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            
            # ç®€åŒ–çš„é™çº§å¤„ç†
            try:
                logger.info(f"[CHAT_SIMPLE_FALLBACK] å°è¯•ç®€å•é™çº§")
                fallback_response = self.model.generate_content(
                    f"è¯·å›ç­”ï¼š{message}",
                    generation_config={"temperature": 0.8, "max_output_tokens": 512}
                )
                
                if fallback_response and fallback_response.text:
                    logger.info(f"[CHAT_SIMPLE_SUCCESS] ç®€å•é™çº§æˆåŠŸ - å“åº”é•¿åº¦: {len(fallback_response.text)} å­—ç¬¦")
                    chunks = self._smart_chunk_text(fallback_response.text)
                    for i, chunk in enumerate(chunks):
                        validated_chunk = self._validate_stream_chunk(chunk)
                        if validated_chunk:
                            logger.debug(f"[CHAT_SIMPLE_CHUNK] ç®€å•é™çº§chunk #{i+1} - é•¿åº¦: {len(validated_chunk)} å­—ç¬¦")
                            yield {
                                "partial_response": validated_chunk,
                                "done": i == len(chunks) - 1,
                                "fallback_used": True,
                                "type": "content"
                            }
                            await asyncio.sleep(0.005)
                    return
            except Exception as fallback_error:
                logger.error(f"[CHAT_SIMPLE_FAILED] ç®€å•é™çº§ä¹Ÿå¤±è´¥: {str(fallback_error)}")
            
            # æœ€ç»ˆé”™è¯¯å¤„ç†
            yield {
                "error": "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•",
                "done": True,
                "type": "error"
            }

    def _parse_response_content(self, response_text: str) -> List[MessageContent]:
        """è§£æAIå“åº”å†…å®¹ä¸ºå¯Œæ–‡æœ¬æ ¼å¼
        
        å‚æ•°:
            response_text: AIç”Ÿæˆçš„å“åº”æ–‡æœ¬
            
        è¿”å›:
            å¯Œæ–‡æœ¬å†…å®¹åˆ—è¡¨
        """
        try:
            if not response_text or not response_text.strip():
                logger.debug(f"[PARSE_CONTENT] å“åº”æ–‡æœ¬ä¸ºç©º")
                return [{"type": "markdown", "content": ""}]
            
            logger.debug(f"[PARSE_CONTENT] å¼€å§‹è§£æå“åº”å†…å®¹ - é•¿åº¦: {len(response_text)} å­—ç¬¦")
            
            # æ£€æµ‹ä»£ç å—
            code_pattern = r'```(\w+)?\n?(.*?)\n?```'
            code_matches = re.findall(code_pattern, response_text, re.DOTALL)
            
            if code_matches:
                logger.debug(f"[PARSE_CONTENT] æ£€æµ‹åˆ° {len(code_matches)} ä¸ªä»£ç å—")
                
                # åˆ†å‰²æ–‡æœ¬å¹¶å¤„ç†ä»£ç å—
                parts = []
                current_pos = 0
                
                for match in re.finditer(code_pattern, response_text, re.DOTALL):
                    # æ·»åŠ ä»£ç å—ä¹‹å‰çš„æ–‡æœ¬
                    before_text = response_text[current_pos:match.start()].strip()
                    if before_text:
                        parts.append({"type": "markdown", "content": before_text})
                    
                    # æ·»åŠ ä»£ç å—
                    language = match.group(1) or "text"
                    code_content = match.group(2).strip()
                    parts.append({
                        "type": "code",
                        "content": code_content,
                        "language": language
                    })
                    
                    current_pos = match.end()
                
                # æ·»åŠ å‰©ä½™æ–‡æœ¬
                remaining_text = response_text[current_pos:].strip()
                if remaining_text:
                    parts.append({"type": "markdown", "content": remaining_text})
                
                return parts
            
            # æ£€æµ‹è¡¨æ ¼ï¼ˆç®€å•çš„ç®¡é“åˆ†éš”è¡¨æ ¼ï¼‰
            table_pattern = r'\|.*?\|.*?\n(?:\|.*?\|.*?\n)+'
            table_matches = re.findall(table_pattern, response_text, re.MULTILINE)
            
            if table_matches:
                logger.debug(f"[PARSE_CONTENT] æ£€æµ‹åˆ° {len(table_matches)} ä¸ªè¡¨æ ¼")
                
                # ç®€åŒ–å¤„ç†ï¼šæš‚æ—¶ä½œä¸ºmarkdownè¿”å›
                return [{"type": "markdown", "content": response_text}]
            
            # é»˜è®¤ä½œä¸ºmarkdownå¤„ç†
            logger.debug(f"[PARSE_CONTENT] ä½œä¸ºmarkdownå†…å®¹å¤„ç†")
            return [{"type": "markdown", "content": response_text}]
            
        except Exception as e:
            logger.error(f"[PARSE_CONTENT_ERROR] å†…å®¹è§£æå¤±è´¥: {str(e)}")
            # é™çº§å¤„ç†ï¼šè¿”å›åŸå§‹æ–‡æœ¬
            return [{"type": "markdown", "content": response_text or ""}]


#0531 æ–°å¢ç½‘é¡µåˆ†ææ–¹æ³•
    async def analyze_web_content(self, question: str, web_context: dict, history: str = None) -> dict:
        """åˆ†æç½‘é¡µå†…å®¹ - æ–°å¢æ–¹æ³•"""
        try:
            logger.info(f"[AI_WEB_ANALYZE] å¼€å§‹åˆ†æç½‘é¡µå†…å®¹")
            
            # æ„å»ºç½‘é¡µä¸“ç”¨prompt
            web_prompt = self._build_web_prompt(question, web_context, history)
            
            # è°ƒç”¨Geminiåˆ†æ
            response = await self.model.generate_content_async(web_prompt)
            response_text = response.text
            
            # æå–ç½‘é¡µä¿¡æ¯
            web_info = self._extract_web_info(web_context)
            
            # è§£æå“åº”å†…å®¹
            parsed_content = self._parse_response_content(response_text)
            
            result = {
                "answer": response_text,
                "sources": web_context.get("chunks", [])[:5],  # é™åˆ¶æ¥æºæ•°é‡
                "confidence": 0.85,  # ç½‘é¡µåˆ†æé€šå¸¸ç½®ä¿¡åº¦è¾ƒé«˜
                "reply": parsed_content,
                "web_info": web_info,
                "analysis_type": "web_content"
            }
            
            logger.info(f"[AI_WEB_ANALYZE_SUCCESS] ç½‘é¡µå†…å®¹åˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"[AI_WEB_ANALYZE_ERROR] ç½‘é¡µå†…å®¹åˆ†æå¤±è´¥: {str(e)}")
            raise Exception(f"ç½‘é¡µå†…å®¹åˆ†æå¤±è´¥: {str(e)}")

    def _build_web_prompt(self, question: str, web_context: dict, history: str = None) -> str:
        """æ„å»ºç½‘é¡µåˆ†æä¸“ç”¨prompt"""
        try:
            # æå–ç½‘é¡µåŸºæœ¬ä¿¡æ¯
            web_info = self._extract_web_info(web_context)
            
            # æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬
            context_text = ""
            if web_context.get("chunks"):
                for i, chunk in enumerate(web_context["chunks"][:8]):  # æœ€å¤š8ä¸ªç‰‡æ®µ
                    chunk_content = chunk.get("content", "")
                    source_url = chunk.get("source_url", "")
                    context_text += f"ã€ç½‘é¡µç‰‡æ®µ {i+1}ã€‘\n{chunk_content}\næ¥æº: {source_url}\n\n"
            
            # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
            history_context = ""
            if history:
                history_context = f"\n\n## ğŸ“– å¯¹è¯å†å²\n{history}\n"
            
            # ç½‘é¡µåˆ†æä¸“ç”¨prompt
            prompt = f"""# ğŸŒ ç½‘é¡µå†…å®¹æ™ºèƒ½åˆ†æåŠ©æ‰‹

## ğŸ“‹ ä»»åŠ¡è¯´æ˜
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µå†…å®¹åˆ†æä¸“å®¶ï¼Œéœ€è¦åŸºäºæä¾›çš„ç½‘é¡µå†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

## ğŸ” ç½‘é¡µä¿¡æ¯
- **æ ‡é¢˜**: {web_info.get('title', 'æœªçŸ¥')}
- **URL**: {web_info.get('url', 'æœªçŸ¥')}
- **å†…å®¹ç±»å‹**: ç½‘é¡µæ–‡æ¡£
- **åˆ†ææ—¶é—´**: {web_info.get('analyzed_at', 'æœªçŸ¥')}

## ğŸ“„ ç½‘é¡µå†…å®¹ç‰‡æ®µ
{context_text}

{history_context}

## â“ ç”¨æˆ·é—®é¢˜
{question}

## ğŸ“‹ åˆ†æè¦æ±‚
1. **ğŸ¯ å‡†ç¡®å›ç­”**: åŸºäºç½‘é¡µå†…å®¹å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜
2. **ğŸ”— å¼•ç”¨æ¥æº**: å¼•ç”¨å…·ä½“çš„ç½‘é¡µç‰‡æ®µå’ŒURL
3. **ğŸ“Š ç»“æ„åŒ–è¾“å‡º**: ä½¿ç”¨æ¸…æ™°çš„æ ¼å¼ç»„ç»‡ç­”æ¡ˆ
4. **ğŸŒ ç½‘é¡µç‰¹æ€§**: è€ƒè™‘ç½‘é¡µå†…å®¹çš„å®æ—¶æ€§å’Œé“¾æ¥æ€§
5. **ğŸ’¡ æ·±åº¦åˆ†æ**: æä¾›æœ‰ä»·å€¼çš„æ´å¯Ÿå’Œæ€»ç»“

## ğŸ¨ å›ç­”æ ¼å¼
è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š

### ğŸ“ **é—®é¢˜è§£ç­”**
[åŸºäºç½‘é¡µå†…å®¹çš„ç›´æ¥å›ç­”]

### ğŸ” **è¯¦ç»†åˆ†æ**
[æ·±å…¥åˆ†æå’Œè§£é‡Š]

### ğŸ”— **ç›¸å…³é“¾æ¥**
[å¦‚æœæœ‰ç›¸å…³é“¾æ¥ï¼Œåˆ—å‡ºæ¥]

### ğŸ’¡ **æ€»ç»“å»ºè®®**
[åŸºäºç½‘é¡µå†…å®¹çš„æ€»ç»“å’Œå»ºè®®]

è¯·å¼€å§‹ä½ çš„åˆ†æï¼š"""

            logger.debug(f"[AI_WEB_PROMPT] ç½‘é¡µåˆ†æpromptæ„å»ºå®Œæˆ")
            return prompt
            
        except Exception as e:
            logger.error(f"[AI_WEB_PROMPT_ERROR] ç½‘é¡µpromptæ„å»ºå¤±è´¥: {str(e)}")
            # è¿”å›åŸºç¡€prompt
            return f"è¯·åŸºäºä»¥ä¸‹ç½‘é¡µå†…å®¹å›ç­”é—®é¢˜ï¼š\n\n{question}\n\nç½‘é¡µå†…å®¹ï¼š\n{web_context}"

    def _extract_web_info(self, web_context: dict) -> dict:
        """æå–ç½‘é¡µä¿¡æ¯"""
        try:
            web_info = {
                "title": "æœªçŸ¥ç½‘é¡µ",
                "url": "æœªçŸ¥",
                "analyzed_at": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S"),
                "chunk_count": 0,
                "content_length": 0
            }
            
            # ä»chunksä¸­æå–ä¿¡æ¯
            if web_context.get("chunks"):
                chunks = web_context["chunks"]
                web_info["chunk_count"] = len(chunks)
                
                # æå–URLå’Œæ ‡é¢˜
                for chunk in chunks:
                    if chunk.get("source_url"):
                        web_info["url"] = chunk["source_url"]
                        break
                
                # è®¡ç®—æ€»å†…å®¹é•¿åº¦
                total_length = sum(len(chunk.get("content", "")) for chunk in chunks)
                web_info["content_length"] = total_length
            
            # ä»æ–‡æ¡£ä¿¡æ¯ä¸­æå–æ ‡é¢˜
            if web_context.get("document_name"):
                web_info["title"] = web_context["document_name"]
            elif web_context.get("text"):
                # ä»å†…å®¹ä¸­æå–æ ‡é¢˜ï¼ˆå–å‰50ä¸ªå­—ç¬¦ï¼‰
                content = web_context["text"][:50]
                web_info["title"] = content.split('\n')[0] if content else "æœªçŸ¥ç½‘é¡µ"
            
            return web_info
            
        except Exception as e:
            logger.error(f"[AI_WEB_INFO_ERROR] æå–ç½‘é¡µä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"title": "æœªçŸ¥ç½‘é¡µ", "url": "æœªçŸ¥", "analyzed_at": "æœªçŸ¥"}

    async def analyze_web_vs_document(self, question: str, web_context: dict, 
                                     doc_context: dict, history: str = None) -> dict:
        """ç½‘é¡µä¸æ–‡æ¡£å¯¹æ¯”åˆ†æ - æ–°å¢æ–¹æ³•"""
        try:
            logger.info(f"[AI_WEB_DOC_COMPARE] å¼€å§‹ç½‘é¡µä¸æ–‡æ¡£å¯¹æ¯”åˆ†æ")
            
            # æ„å»ºå¯¹æ¯”åˆ†æprompt
            compare_prompt = self._build_web_doc_compare_prompt(
                question, web_context, doc_context, history
            )
            
            # è°ƒç”¨Geminiåˆ†æ
            response = await self.model.generate_content_async(compare_prompt)
            response_text = response.text
            
            # æå–ä¿¡æ¯
            web_info = self._extract_web_info(web_context)
            doc_info = self._extract_document_info(doc_context)
            
            # è§£æå“åº”å†…å®¹
            parsed_content = self._parse_response_content(response_text)
            
            # åˆå¹¶æ¥æº
            combined_sources = []
            if web_context.get("chunks"):
                for chunk in web_context["chunks"][:3]:
                    chunk_copy = chunk.copy()
                    chunk_copy["source_type"] = "web"
                    combined_sources.append(chunk_copy)
            
            if doc_context.get("chunks"):
                for chunk in doc_context["chunks"][:3]:
                    chunk_copy = chunk.copy()
                    chunk_copy["source_type"] = "document"
                    combined_sources.append(chunk_copy)
            
            result = {
                "answer": response_text,
                "sources": combined_sources,
                "confidence": 0.9,  # å¯¹æ¯”åˆ†æç½®ä¿¡åº¦æ›´é«˜
                "reply": parsed_content,
                "web_info": web_info,
                "doc_info": doc_info,
                "analysis_type": "web_document_comparison"
            }
            
            logger.info(f"[AI_WEB_DOC_COMPARE_SUCCESS] ç½‘é¡µä¸æ–‡æ¡£å¯¹æ¯”åˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"[AI_WEB_DOC_COMPARE_ERROR] ç½‘é¡µä¸æ–‡æ¡£å¯¹æ¯”åˆ†æå¤±è´¥: {str(e)}")
            raise Exception(f"ç½‘é¡µä¸æ–‡æ¡£å¯¹æ¯”åˆ†æå¤±è´¥: {str(e)}")

    def _build_web_doc_compare_prompt(self, question: str, web_context: dict, 
                                     doc_context: dict, history: str = None) -> str:
        """æ„å»ºç½‘é¡µä¸æ–‡æ¡£å¯¹æ¯”åˆ†æprompt"""
        try:
            # æå–ä¿¡æ¯
            web_info = self._extract_web_info(web_context)
            doc_info = self._extract_document_info(doc_context)
            
            # æ„å»ºç½‘é¡µå†…å®¹
            web_content = ""
            if web_context.get("chunks"):
                for i, chunk in enumerate(web_context["chunks"][:5]):
                    web_content += f"ã€ç½‘é¡µç‰‡æ®µ {i+1}ã€‘\n{chunk.get('content', '')}\n\n"
            
            # æ„å»ºæ–‡æ¡£å†…å®¹
            doc_content = ""
            if doc_context.get("chunks"):
                for i, chunk in enumerate(doc_context["chunks"][:5]):
                    doc_content += f"ã€æ–‡æ¡£ç‰‡æ®µ {i+1}ã€‘\n{chunk.get('content', '')}\n\n"
            
            # å†å²å¯¹è¯
            history_context = f"\n\n## ğŸ“– å¯¹è¯å†å²\n{history}\n" if history else ""
            
            prompt = f"""# ğŸ”„ ç½‘é¡µä¸æ–‡æ¡£å¯¹æ¯”åˆ†æåŠ©æ‰‹

## ğŸ“‹ ä»»åŠ¡è¯´æ˜
ä½ éœ€è¦å¯¹æ¯”åˆ†æç½‘é¡µå†…å®¹å’Œæ–‡æ¡£å†…å®¹ï¼Œä¸ºç”¨æˆ·æä¾›ç»¼åˆæ€§çš„ç­”æ¡ˆã€‚

## ğŸŒ ç½‘é¡µä¿¡æ¯
- **æ ‡é¢˜**: {web_info.get('title', 'æœªçŸ¥')}
- **URL**: {web_info.get('url', 'æœªçŸ¥')}
- **ç±»å‹**: ç½‘é¡µå†…å®¹

### ç½‘é¡µå†…å®¹ï¼š
{web_content}

## ğŸ“„ æ–‡æ¡£ä¿¡æ¯  
- **æ–‡æ¡£å**: {doc_info.get('title', 'æœªçŸ¥')}
- **ç±»å‹**: {doc_info.get('type', 'æ–‡æ¡£')}

### æ–‡æ¡£å†…å®¹ï¼š
{doc_content}

{history_context}

## â“ ç”¨æˆ·é—®é¢˜
{question}

## ğŸ“‹ å¯¹æ¯”åˆ†æè¦æ±‚
1. **ğŸ” å†…å®¹å¯¹æ¯”**: æ¯”è¾ƒç½‘é¡µå’Œæ–‡æ¡£ä¸­çš„ç›¸å…³ä¿¡æ¯
2. **âœ… ä¸€è‡´æ€§åˆ†æ**: æ‰¾å‡ºä¿¡æ¯çš„ä¸€è‡´æ€§å’Œå·®å¼‚æ€§
3. **â° æ—¶æ•ˆæ€§è€ƒé‡**: è€ƒè™‘ç½‘é¡µå†…å®¹çš„å®æ—¶æ€§ä¼˜åŠ¿
4. **ğŸ“Š æƒå¨æ€§è¯„ä¼°**: è¯„ä¼°ä¸åŒæ¥æºçš„å¯é æ€§
5. **ğŸ’¡ ç»¼åˆå»ºè®®**: æä¾›åŸºäºä¸¤ç§æ¥æºçš„ç»¼åˆå»ºè®®

## ğŸ¨ å›ç­”æ ¼å¼
### ğŸ“ **ç»¼åˆå›ç­”**
[åŸºäºç½‘é¡µå’Œæ–‡æ¡£çš„ç»¼åˆç­”æ¡ˆ]

### ğŸ” **å¯¹æ¯”åˆ†æ**
| å¯¹æ¯”ç»´åº¦ | ç½‘é¡µå†…å®¹ | æ–‡æ¡£å†…å®¹ | åˆ†æè¯´æ˜ |
|---------|---------|---------|---------|
| ä¿¡æ¯ä¸€è‡´æ€§ | ... | ... | ... |
| è¯¦ç»†ç¨‹åº¦ | ... | ... | ... |
| æ—¶æ•ˆæ€§ | ... | ... | ... |

### ğŸŒ **ç½‘é¡µä¼˜åŠ¿**
- [ç½‘é¡µå†…å®¹çš„ç‹¬ç‰¹ä»·å€¼]

### ğŸ“„ **æ–‡æ¡£ä¼˜åŠ¿**  
- [æ–‡æ¡£å†…å®¹çš„ç‹¬ç‰¹ä»·å€¼]

### ğŸ’¡ **ç»¼åˆå»ºè®®**
[åŸºäºä¸¤ç§æ¥æºçš„æœ€ç»ˆå»ºè®®]

è¯·å¼€å§‹ä½ çš„å¯¹æ¯”åˆ†æï¼š"""

            logger.debug(f"[AI_WEB_DOC_PROMPT] å¯¹æ¯”åˆ†æpromptæ„å»ºå®Œæˆ")
            return prompt
            
        except Exception as e:
            logger.error(f"[AI_WEB_DOC_PROMPT_ERROR] å¯¹æ¯”promptæ„å»ºå¤±è´¥: {str(e)}")
            return f"è¯·å¯¹æ¯”åˆ†æä»¥ä¸‹ç½‘é¡µå’Œæ–‡æ¡£å†…å®¹æ¥å›ç­”é—®é¢˜ï¼š\n\n{question}"

    async def identify_web_intent(self, question: str, web_context: dict = None) -> str:
        """è¯†åˆ«ç½‘é¡µç›¸å…³çš„æ„å›¾ - æ–°å¢æ–¹æ³•"""
        try:
            logger.info(f"[AI_WEB_INTENT] å¼€å§‹è¯†åˆ«ç½‘é¡µç›¸å…³æ„å›¾")
            
            # ç½‘é¡µç‰¹å®šçš„æ„å›¾æ¨¡å¼
            web_patterns = {
                "WEB_LINK_ANALYSIS": [
                    "é“¾æ¥", "è¶…é“¾æ¥", "è·³è½¬", "ç›¸å…³é“¾æ¥", "å‚è€ƒé“¾æ¥", "å¤–éƒ¨é“¾æ¥"
                ],
                "WEB_CONTENT_EXTRACT": [
                    "æå–", "æ‘˜è¦", "æ€»ç»“", "å…³é”®ä¿¡æ¯", "ä¸»è¦å†…å®¹"
                ],
                "WEB_STRUCTURE_ANALYSIS": [
                    "ç»“æ„", "å¸ƒå±€", "ç»„ç»‡", "ç« èŠ‚", "ç›®å½•", "å¯¼èˆª"
                ],
                "WEB_COMPARISON": [
                    "å¯¹æ¯”", "æ¯”è¾ƒ", "å·®å¼‚", "ç›¸åŒ", "ä¸åŒ", "å¼‚åŒ"
                ],
                "WEB_REAL_TIME": [
                    "æœ€æ–°", "å®æ—¶", "å½“å‰", "ç°åœ¨", "æœ€è¿‘", "æ›´æ–°"
                ]
            }
        
            question_lower = question.lower()
            
            # æ¨¡å¼åŒ¹é…
            for intent, patterns in web_patterns.items():
                if any(pattern in question_lower for pattern in patterns):
                    logger.info(f"[AI_WEB_INTENT_FOUND] è¯†åˆ«åˆ°ç½‘é¡µæ„å›¾: {intent}")
                    return intent
            
            # å¦‚æœæœ‰ç½‘é¡µä¸Šä¸‹æ–‡ï¼Œé»˜è®¤ä¸ºç½‘é¡µæŸ¥è¯¢
            if web_context and web_context.get("chunks"):
                logger.info(f"[AI_WEB_INTENT_DEFAULT] é»˜è®¤ç½‘é¡µæŸ¥è¯¢æ„å›¾")
                return "WEB_QUERY"
            
            logger.info(f"[AI_WEB_INTENT_GENERAL] é€šç”¨æ„å›¾")
            return "GENERAL_QUERY"
            
        except Exception as e:
            logger.error(f"[AI_WEB_INTENT_ERROR] ç½‘é¡µæ„å›¾è¯†åˆ«å¤±è´¥: {str(e)}")
            return "GENERAL_QUERY"